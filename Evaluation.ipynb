{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3d5a5d-ccd7-4cf5-bc63-fb0a75078498",
   "metadata": {},
   "source": [
    "# Document Retrieval\n",
    "This project focuses on document retrieval, a core task in information retrieval where the goal is to rank documents by their relevance to a given query. Using a dataset containing over 199,000 documents across multiple languages, the aim is to implement a model that can effectively retrieve the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd759e38-1c19-49cd-a556-fdc9fa3fba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f904929-9d06-4a3f-83be-b5eb5cb5e584",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df33238-a82a-4591-8e5f-43ddbcce410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text Preprocessing\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Counting and Data Structures\n",
    "from collections import Counter\n",
    "\n",
    "# Similarity Computation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sparse Matrices\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912d4c8-40e9-4dab-b61e-7258aae2fdf4",
   "metadata": {},
   "source": [
    "Before we start, we need to import the pre-computed BM25 matrices, IDF dictionaries and vocabularies for all languages calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4fca4-0524-45f8-8852-2198e1427e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for each language's precomputed data\n",
    "data_paths = {\n",
    "    'en': {\n",
    "        'documents': 'Data/corpus_en_processed.csv',\n",
    "        'idf': 'Data/idf_dict_en.pkl',\n",
    "        'vocabulary': 'Data/bm25_vocabulary_en.pkl',\n",
    "        'bm25': 'Data/bm25_matrix_en.npz'\n",
    "    },\n",
    "    'fr': {\n",
    "        'documents': 'Data/corpus_fr_processed.csv',\n",
    "        'idf': 'Data/idf_dict_fr.pkl',\n",
    "        'vocabulary': 'Data/bm25_vocabulary_fr.pkl',\n",
    "        'bm25': 'Data/bm25_matrix_fr.npz'\n",
    "    },\n",
    "    'de': {\n",
    "        'documents': 'Data/corpus_de_processed.csv',\n",
    "        'idf': 'Data/idf_dict_de.pkl',\n",
    "        'vocabulary': 'Data/bm25_vocabulary_de.pkl',\n",
    "        'bm25': 'Data/bm25_matrix_de.npz'\n",
    "    },\n",
    "    'es': {\n",
    "        'documents': 'Data/corpus_es_processed.csv',\n",
    "        'idf': 'Data/idf_dict_es.pkl',\n",
    "        'vocabulary': 'Data/bm25_vocabulary_es.pkl',\n",
    "        'bm25': 'Data/bm25_matrix_es.npz'\n",
    "    },\n",
    "    'it': {\n",
    "        'documents': 'Data/corpus_it_processed.csv',\n",
    "        'idf': 'Data/idf_dict_it.pkl',\n",
    "        'vocabulary': 'Data/bm25_vocabulary_it.pkl',\n",
    "        'bm25': 'Data/bm25_matrix_it.npz'\n",
    "    },  \n",
    "    'ar': {\n",
    "        'documents': 'Data/corpus_ar_processed.csv',\n",
    "        'idf': 'Data/idf_dict_ar.pkl',\n",
    "        'vocabulary': 'Data/bm25_vocabulary_ar.pkl',\n",
    "        'bm25': 'Data/bm25_matrix_ar.npz'\n",
    "    },\n",
    "    'ko': {\n",
    "        'documents': 'Data/corpus_ko_processed.csv',\n",
    "        'idf': 'Data/idf_dict_ko.pkl',\n",
    "        'vocabulary': 'Data/bm25_vocabulary_ko.pkl',\n",
    "        'bm25': 'Data/bm25_matrix_ko.npz'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store loaded data for each language\n",
    "documents = {}\n",
    "idf_dicts = {}\n",
    "vocabularies = {}\n",
    "bm25_matrices = {}\n",
    "\n",
    "# Load the data for each language\n",
    "for lang, paths in data_paths.items():\n",
    "\n",
    "    # Load documents\n",
    "    documents[lang] = pd.read_csv(paths['documents'])\n",
    "\n",
    "    # Load IDF dictionary\n",
    "    idf_dicts[lang] = pd.read_pickle(paths['idf'])\n",
    "\n",
    "    # Load vocabulary\n",
    "    vocabularies[lang] = pd.read_pickle(paths['vocabulary'])\n",
    "\n",
    "    # Load bm25 matrix\n",
    "    bm25_matrices[lang] = sp.load_npz(paths['bm25'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42e3b0-d35f-44b2-83e4-8de309bbeffa",
   "metadata": {},
   "source": [
    "Now, let's load the file containing the queries for which we need to retrieve the top-10 relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf98e31-a100-499c-bab0-5e561096406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing queries (test set)\n",
    "dev_path = 'Data/dev.csv'\n",
    "dev_df = pd.read_csv(dev_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a5c6a-aefd-4306-a335-08e2945296d2",
   "metadata": {},
   "source": [
    "The following will be used to process all queries in the DataFrame in the same way the documents were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150db01-1942-4432-a14f-d862f065a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords for each language\n",
    "stopwords_dict = {\n",
    "    'en': set(stopwords.words('english')),\n",
    "    'fr': set(stopwords.words('french')),\n",
    "    'de': set(stopwords.words('german')),\n",
    "    'es': set(stopwords.words('spanish')),\n",
    "    'it': set(stopwords.words('italian')),\n",
    "    'ar': set(stopwords.words('arabic'))\n",
    "}\n",
    "\n",
    "# Load Korean stopwords from an external file\n",
    "with open('Data/stopwords-ko.txt', 'r', encoding='utf-8') as f: # /kaggle/input/korean-stop-words/stopwords-ko.txt\n",
    "    stopwords_dict['ko'] = set(f.read().splitlines())\n",
    "\n",
    "# Define stemmers for each language\n",
    "stemmer_dict = {\n",
    "    'en': SnowballStemmer('english'),\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'de': SnowballStemmer('german'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'it': SnowballStemmer('italian'),\n",
    "    'ar': None,  # No stemmer for Arabic\n",
    "    'ko': None   # No stemmer for Korean\n",
    "}\n",
    "\n",
    "# Function to apply stemming based on language\n",
    "def apply_stemming(tokens, lang):\n",
    "    stemmer = stemmer_dict.get(lang, None)\n",
    "    if stemmer:  # Apply stemming only if a stemmer is available for the language\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    return tokens  # If no stemmer, return tokens as-is\n",
    "\n",
    "# Preprocessing function for each document based on its language\n",
    "def preprocess_single_text(text, lang):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Retain only alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()] \n",
    "    \n",
    "    # Remove stopwords based on the language\n",
    "    stop_words = stopwords_dict.get(lang, set())  \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    tokens = apply_stemming(tokens, lang)\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33322ebd-5dcc-468d-893f-0b63b154c461",
   "metadata": {},
   "source": [
    "We also need to create the function that process the query and returns the top 10 most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067f094-8188-4e35-986f-c0ffbde75129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_bm25(query, lang, bm25_matrix, idf_dict, vocabulary, top_n=10):\n",
    "    # Preprocess the query\n",
    "    preprocessed_query = preprocess_single_text(query, lang)\n",
    "    \n",
    "    # Tokenize the preprocessed query\n",
    "    query_tokens = preprocessed_query.split()\n",
    "    \n",
    "    # Initialize the query vector with zeros\n",
    "    query_vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    # Fill in the IDF values for the query vector\n",
    "    for term in set(query_tokens):\n",
    "        if term in idf_dict:\n",
    "            term_index = vocabulary.index(term)\n",
    "            query_vector[term_index] = idf_dict[term]\n",
    "    \n",
    "    # Reshape the query vector to a 2D array (needed for cosine similarity)\n",
    "    query_vector = query_vector.reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_vector, bm25_matrix).flatten()\n",
    "    \n",
    "    # Get the indices of the top N most similar documents\n",
    "    top_n_indices = np.argsort(similarities)[-top_n:][::-1]  # Sort in descending order of similarity\n",
    "    \n",
    "    return top_n_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7594f43-d0e6-497e-b5d7-a010d564e981",
   "metadata": {},
   "source": [
    "Finally, all that remains for us to do is to process each query and store the corresponding top 10 document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a94de-b02b-426a-8907-dad70beb0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each query in the test_df and store the top 10 document IDs for each query\n",
    "results = []\n",
    "\n",
    "# Wrap the iteration with tqdm to add a progress bar\n",
    "for idx, row in tqdm(dev_df.iterrows(), total=len(dev_df), desc=\"Processing Queries\"):\n",
    "    query = row['query']\n",
    "    lang = row['lang']\n",
    "    \n",
    "    # Select the appropriate BM25 matrix, IDF dictionary, and vocabulary based on the language\n",
    "    bm25_matrix = bm25_matrices[lang]\n",
    "    idf_dict = idf_dicts[lang]\n",
    "    vocabulary = vocabularies[lang]\n",
    "    \n",
    "    # Get the top 10 most relevant documents for the query\n",
    "    top_n_doc_indices = process_query_bm25(query, lang, bm25_matrix, idf_dict, vocabulary, top_n=10)\n",
    "    \n",
    "    # Retrieve the document IDs based on the indices from the corresponding documents DataFrame\n",
    "    top_n_docids = documents[lang].iloc[top_n_doc_indices]['docid'].tolist()\n",
    "    \n",
    "    # Convert the document IDs to the required format (e.g., 'doc-en-7459')\n",
    "    formatted_docids = [f'{docid}' for docid in top_n_docids]\n",
    "    \n",
    "    # Append the result as a new row (index, docids)\n",
    "    results.append({'id': idx, 'docids': str(formatted_docids)})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51f4a6-be78-4703-94dd-604d0d5e3548",
   "metadata": {},
   "source": [
    "Thus, we generated the submission which is a csv file having the following two columns: 'id', 'docids'. The column 'id' refers to the id of the query in the test.csv and the columnn 'docids' refers to the list of retrieved document ids. In order to evaluate our model, we need to check whether the identifier of the positive document is among the first 10 documents returned by our model for each query. If so, the function returns 1, otherwise it returns 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294cb13-e22e-4f63-860b-a82ea501d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Recall@10\n",
    "def calculate_recall_at_10(dev_row, top_10_docs):\n",
    "    \"\"\"\n",
    "    This function calculates Recall@10 for a specific query.\n",
    "    dev_row : row from the dev dataframe containing the query details\n",
    "    top_10_docs : list of the top 10 relevant documents from results_df for the query\n",
    "    \"\"\"\n",
    "    # Get the positive document ID from dev\n",
    "    positive_doc = dev_row['positive_docs']\n",
    "    # Check if the positive document is among the top 10 documents\n",
    "    if positive_doc in top_10_docs:\n",
    "        return 1  # Success\n",
    "    else:\n",
    "        return 0  # Failure\n",
    "\n",
    "# Calculate Recall@10 for all queries\n",
    "dev_df['recall_at_10'] = dev_df.apply(lambda row: calculate_recall_at_10(row, results_df.iloc[row.name]['docids']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1ed5a-8d34-4486-a1d2-1aff15317667",
   "metadata": {},
   "source": [
    "All that remains is to calculate the average Recall@10 for each language by grouping queries according to language, and to calculate the overall average Recall@10 for all languages across the entire database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f9a5a-e694-43d6-9ca6-f30348fb5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Recall@10 per Language\n",
    "recall_by_lang = dev_df.groupby('lang')['recall_at_10'].mean()\n",
    "\n",
    "# Display Recall@10 per language\n",
    "print(\"\\nRecall@10 per language:\")\n",
    "print(recall_by_lang)\n",
    "\n",
    "# Calculate Overall Average Recall@10 \n",
    "overall_recall = dev_df['recall_at_10'].mean()\n",
    "\n",
    "# Display the overall Recall@10\n",
    "print(\"\\nOverall Recall@10:\", overall_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee632ba-e6a4-4e16-994f-0d0e8508233e",
   "metadata": {},
   "source": [
    "These scores give us a more in-depth idea of ​​the performance of our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
