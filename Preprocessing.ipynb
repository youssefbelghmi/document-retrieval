{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dc4288-0610-4290-bd18-445533174651",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "\n",
    "In text retrieval, preprocessing is a vital step for enhancing the quality and relevance of search results. The goal of text retrieval is to find documents that are most relevant to a user's query. Raw text data, however, is often noisy and inconsistent, which can hinder the retrieval of the most relevant documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d14d9-9f46-45c0-93a2-d2c09f655288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7978d-16f7-4832-81cc-ad000df87819",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9099a8-b9b6-4a96-9b34-dc52e549393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK libraries for tokenization, stopwords, and stemming\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Ensure that NLTK's punkt tokenizer is available\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ded7c3-1c6d-4b72-8dca-d9b2cda22294",
   "metadata": {},
   "source": [
    "The goal of this notebook is to load the document corpus and preprocess them to improve the efficiency of our document retrieval system. The corpus contains documents in seven different languages, and we aim to process each document separately based on its language. The result of this preprocessing step will therefore be seven separate CSV files, one for each language in the corpus. Each CSV file will contain the documents of the corresponding language, but with the processed text. These preprocessed text files will serve as the basis for our document retrieval system. Before we begin, let's load the document corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a23e8-7958-4742-aa26-f59cd6b58cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON corpus from the specified path\n",
    "corpus_path = 'Data/corpus.json'\n",
    "with open(corpus_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the JSON corpus into a Pandas DataFrame\n",
    "corpus_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba741228-46f5-4e6b-b69e-70c16322647f",
   "metadata": {},
   "source": [
    "The first step is to split our corpus so that we end up with seven different dataframes, each corresponding to the documents in the corpus in a certain language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33612a53-ecf4-4599-a4b4-d56ba0a70441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique set of languages in the corpus\n",
    "languages = set(corpus_df['lang'].tolist())\n",
    "\n",
    "# Create separate DataFrames for each language\n",
    "corpus_en = corpus_df[corpus_df['lang'] == 'en']\n",
    "corpus_fr = corpus_df[corpus_df['lang'] == 'fr']\n",
    "corpus_de = corpus_df[corpus_df['lang'] == 'de']\n",
    "corpus_es = corpus_df[corpus_df['lang'] == 'es']\n",
    "corpus_it = corpus_df[corpus_df['lang'] == 'it']\n",
    "corpus_ar = corpus_df[corpus_df['lang'] == 'ar']\n",
    "corpus_ko = corpus_df[corpus_df['lang'] == 'ko']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92ffe8-8fed-46e2-934f-3a4bc5bca051",
   "metadata": {},
   "source": [
    "Since the corpus is multilingual, preprocessing must be language-specific. So, we have to define stopwords and stemmers for each supported language in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c4c36-e43e-414f-af57-7b2cbd4a3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords for each language\n",
    "stopwords_dict = {\n",
    "    'en': set(stopwords.words('english')),\n",
    "    'fr': set(stopwords.words('french')),\n",
    "    'de': set(stopwords.words('german')),\n",
    "    'es': set(stopwords.words('spanish')),\n",
    "    'it': set(stopwords.words('italian')),\n",
    "    'ar': set(stopwords.words('arabic'))\n",
    "}\n",
    "\n",
    "# Load Korean stopwords from an external file\n",
    "with open('Data/stopwords-ko.txt', 'r', encoding='utf-8') as f: # /kaggle/input/korean-stop-words/stopwords-ko.txt\n",
    "    stopwords_dict['ko'] = set(f.read().splitlines())\n",
    "\n",
    "# Define stemmers for each language\n",
    "stemmer_dict = {\n",
    "    'en': SnowballStemmer('english'),\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'de': SnowballStemmer('german'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'it': SnowballStemmer('italian'),\n",
    "    'ar': None,  # No stemmer for Arabic\n",
    "    'ko': None   # No stemmer for Korean\n",
    "}\n",
    "\n",
    "# Function to apply stemming based on language\n",
    "def apply_stemming(tokens, lang):\n",
    "    stemmer = stemmer_dict.get(lang, None)\n",
    "    if stemmer:  # Apply stemming only if a stemmer is available for the language\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    return tokens  # If no stemmer, return tokens as-is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785f613-11e8-4607-a243-12ad34f0bb5f",
   "metadata": {},
   "source": [
    "Now, we have to implement the functions that will allow us to preprocess each document in our corpus according to its language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bf93a-fdba-4c7c-a8da-654bba6a011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for each document based on its language\n",
    "def preprocess_single_text(text, lang):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Retain only alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()] \n",
    "    \n",
    "    # Remove stopwords based on the language\n",
    "    stop_words = stopwords_dict.get(lang, set())  \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    tokens = apply_stemming(tokens, lang)\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Function to preprocess a Pandas DataFrame column\n",
    "def preprocess_pandas(df, lang):\n",
    "    # Add a counter to track progress\n",
    "    counter = 0\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Function to process each row and print progress every 100 rows\n",
    "    def process_row(text):\n",
    "        nonlocal counter\n",
    "        counter += 1\n",
    "        \n",
    "        # Print progress every 100 rows\n",
    "        if counter % 100 == 0 or counter == total_rows:\n",
    "            print(f\"Processed {counter}/{total_rows} rows\")\n",
    "        \n",
    "        return preprocess_single_text(text, lang)\n",
    "    \n",
    "    # Apply the processing function to the DataFrame\n",
    "    df['text'] = df['text'].apply(process_row)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287261d-6d8a-490b-904c-3a30c3f46bd6",
   "metadata": {},
   "source": [
    "Finally, all we have to do is preprocess all the documents for each language and save the results for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08553f-0153-4803-ac6f-035bfb65e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the English corpus\n",
    "lang = 'en'\n",
    "corpus_en_processed = preprocess_pandas(corpus_en, lang)\n",
    "\n",
    "# Show the result\n",
    "print(corpus_en_processed.head())\n",
    "\n",
    "# Save the processed corpus\n",
    "corpus_en_processed.to_csv('Data/corpus_en_processed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284982e-83ab-4358-9ac1-d43e58c33b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the French corpus\n",
    "lang = 'fr'\n",
    "corpus_fr_processed = preprocess_pandas(corpus_fr, lang)\n",
    "\n",
    "# Show the result\n",
    "print(corpus_fr_processed.head())\n",
    "\n",
    "# Save the processed corpus\n",
    "corpus_fr_processed.to_csv('Data/corpus_fr_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f624a5-e710-4c07-b611-b94564ff1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the German corpus\n",
    "lang = 'de'\n",
    "corpus_de_processed = preprocess_pandas(corpus_de, lang)\n",
    "\n",
    "# Show the result\n",
    "print(corpus_de_processed.head())\n",
    "\n",
    "# Save the processed corpus\n",
    "corpus_de_processed.to_csv('Data/corpus_de_processed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26586192-c563-472e-b272-506ceaceb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Spanish corpus\n",
    "lang = 'es'\n",
    "corpus_es_processed = preprocess_pandas(corpus_es, lang)\n",
    "\n",
    "# Show the result\n",
    "print(corpus_es_processed.head())\n",
    "\n",
    "# Save the processed corpus\n",
    "corpus_es_processed.to_csv('Data/corpus_es_processed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5713d4a-e3e9-4b56-8a17-ae090dd06e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Italian corpus\n",
    "lang = 'it'\n",
    "corpus_it_processed = preprocess_pandas(corpus_it, lang)\n",
    "\n",
    "# Show the result\n",
    "print(corpus_it_processed.head())\n",
    "\n",
    "# Save the processed corpus\n",
    "corpus_it_processed.to_csv('Data/corpus_it_processed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d665277-0315-45aa-94ee-fec938ab22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Arabic corpus\n",
    "lang = 'ar'\n",
    "corpus_ar_processed = preprocess_pandas(corpus_ar, lang)\n",
    "\n",
    "# Show the result\n",
    "print(corpus_ar_processed.head())\n",
    "\n",
    "# Save the processed corpus\n",
    "corpus_ar_processed.to_csv('Data/corpus_ar_processed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd666be6-66bd-4952-a43a-9c1ea04bdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Korean corpus\n",
    "lang = 'ko'\n",
    "corpus_ko_processed = preprocess_pandas(corpus_ko, lang)\n",
    "\n",
    "# Show the result\n",
    "print(corpus_ko_processed.head())\n",
    "\n",
    "# Save the processed corpus\n",
    "corpus_ko_processed.to_csv('Data/corpus_ko_processed.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f7d7a-0f33-48bc-a70e-92ebf1dd383d",
   "metadata": {},
   "source": [
    "So, we got our 7 CSV files containing the documents processed according to their language. It is on these files that we will rely to carry out our document retrieval task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
